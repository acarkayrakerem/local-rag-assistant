# ğŸ¤– Local Ollama RAG Assistant

A fully local AI chatbot with a web-based interface, powered by
**Retrieval-Augmented Generation (RAG)** and **Ollama**. This assistant runs
completely offline (This is optional, you can paste your API key and use
frontier models as well) and can answer your questions based on the documents
you upload, including PDFs, Word files, text files, and more. You can get
answers to your questions from very large datasets that are impossible to upload
a frontier model and unsafe to share.

## âœ¨ Features

- ğŸ”’ **100% Local & Private**: Your documents stay on your computer. No data
  needs to leave your machine.
- ğŸ§  **Smart RAG Pipeline**: Built using **LangChain**, **ChromaDB**, and
  **Hugging Face embeddings** to understand your documents.
- ğŸ—‚ï¸ **Multi-Format Support**: Works with PDFs (`.pdf`), Word docs (`.docx`,
  `.doc`), Excel (`.xlsx`, `.xls`), Images (`.jpg`, `.png`), `.txt`, `.md`,
  `.py`, `.json`, `.csv` and `.sql`.
- ğŸ’¬ **Gradio Web Interface**: Clean, easy-to-use chat interface running in your
  browser.
- ğŸ“š **Source Citations**: Tells you exactly which file the answer came from.
- ğŸ§ª **Test Data Included**: Comes with a `test_DB` of synthetic employee data
  to try immediately.
- ğŸ“Š **Built-in Evaluation**: Generate synthetic test questions (**SDG**) and
  evaluate your RAG pipeline's accuracy automatically.

## ğŸ› ï¸ Tech Stack

- **Ollama** (LLM backend, e.g., `llama3.2`)
- **LangChain** (RAG framework)
- **Chroma** (Persistent vector database)
- **HuggingFace Embeddings** (Local embedding models)
- **Gradio** (Frontend UI)

---

## ğŸš€ Getting Started

Follow these steps to set up the project from scratch.

### 1. Prerequisites

- **Python 3.10+** installed on your system.
- **Git** installed.
- **Ollama** (for local/free mode).

### 2. Install and Setup Ollama

1. Download and install **[Ollama](https://ollama.com)**.
   - _Mac Users_: You can also run `brew install ollama`.
2. Open your terminal and pull the model:
   ```bash
   ollama pull llama3.2:latest
   ```
3. **Keep Ollama Application running** in the background.

### 3. Installation

**Clone the repository:**

```bash
git clone https://github.com/acarkayrakerem/local-rag-assistant.git
cd local-rag-assistant
```

**Create a Virtual Environment (Recommended):**

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate
```

**Install Dependencies:**

```bash
pip install -r requirements.txt
```

---

## ğŸƒâ€â™‚ï¸ How to Run

1. **Start the app:**
   ```bash
   python app.py
   ```
2. **Open in Browser:** You will see a message like
   `Running on local URL: http://127.0.0.1:7860`. Click the link to open the
   chat.

## ğŸ’¡ How to Use

1. **Select Provider**:
   - Choose **`ollama(free)`** for unrelated local privacy.
   - Or choose OpenAI/Google/Anthropic if you have an API key.
2. **Select Database**:
   - Click the ğŸ“ File Explorer icon.
   - Navigate to your document folder.
   - **Important**: To select the current folder, click the **folder icon with a
     `.` (dot)** next to it.
3. **Vectorize**:
   - Click **"Vectorize The Database"**. This reads your files and creates the
     "brain" for the AI. No need to do it again unless you want to update the
     database.
   - Ask questions like _"Who has an AWS certificate?"_ and get answers based on
     your files!
4. **Eval & SDG**:
   - Switch to the **SDG & Testing** tab.
   - **Generate Test Set**: Create synthetic question-answer pairs from your
     documents.
   - **Run Evaluation**: Test your model's accuracy against the synthetic data.

## ğŸ§ª Sample Data

Included is a `test_DB` folder with synthetic employee records generated by
**Gemini 2.5 Flash**. You can point the database selector to `test_DB` to test
the app right away!

---

## â“ Troubleshooting

- **`NameError: ChatOllama is not defined`**: Run
  `pip install -r requirements.txt` again to ensure all new dependencies are
  installed.
- **Ollama connection failed**: Ensure the Ollama app is running and you pulled
  `llama3.2`.
- **Missing files**: Check the allowed file types in `ingest.py`.
