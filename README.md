# ğŸ¤– Local Ollama RAG Assistant

A fully local AI chatbot with a web-based interface, powered by
**Retrieval-Augmented Generation (RAG)** and **Ollama**. This assistant runs
completely offline (optional) and can answer your questions based on the
documents you upload â€” including PDFs, Word files, text files, and more.

## âœ¨ Features

- ğŸ”’ **100% Local & Private**: Your documents stay on your computer. No data
  needs to leave your machine.
- ğŸ§  **Smart RAG Pipeline**: Built using **LangChain**, **ChromaDB**, and
  **Hugging Face embeddings** to understand your documents.
- ğŸ—‚ï¸ **Multi-Format Support**: Works with PDFs, Word docs (`.docx`), `.txt`,
  `.md` and more.
- ğŸ’¬ **Gradio Web Interface**: Clean, easy-to-use chat interface running in your
  browser.
- ğŸ“š **Source Citations**: Tells you exactly which file the answer came from.
- ğŸ§ª **Test Data Included**: Comes with a `test_DB` of synthetic employee data
  to try immediately.

## ğŸ› ï¸ Tech Stack

- **Ollama** (LLM backend, e.g., `llama3.2`)
- **LangChain** (RAG framework)
- **Chroma** (Persistent vector database)
- **HuggingFace Embeddings** (Local embedding models)
- **Gradio** (Frontend UI)

---

## ğŸš€ Getting Started

Follow these steps to set up the project from scratch.

### 1. Prerequisites

- **Python 3.10+** installed on your system.
- **Git** installed.
- **Ollama** (for local/free mode).

### 2. Install and Setup Ollama

1. Download and install **[Ollama](https://ollama.com)**.
   - _Mac Users_: You can also run `brew install ollama`.
2. Open your terminal and pull the model:
   ```bash
   ollama pull llama3.2:latest
   ```
3. **Keep Ollama Application running** in the background.

### 3. Installation

**Clone the repository:**

```bash
git clone https://github.com/acarkayrakerem/local-rag-assistant.git
cd local-rag-assistant
```

**Create a Virtual Environment (Recommended):**

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate
```

**Install Dependencies:**

```bash
pip install -r requirements.txt
```

---

## ğŸƒâ€â™‚ï¸ How to Run

1. **Start the app:**
   ```bash
   python app.py
   ```
2. **Open in Browser:** You will see a message like
   `Running on local URL: http://127.0.0.1:7860`. Click the link to open the
   chat.

## ğŸ’¡ How to Use

1. **Select Provider**:
   - Choose **`ollama(free)`** for unrelated local privacy.
   - Or choose OpenAI/Google/Anthropic if you have an API key.
2. **Select Database**:
   - Click the ğŸ“ File Explorer icon.
   - Navigate to your document folder.
   - **Important**: To select the current folder, click the **folder icon with a
     `.` (dot)** next to it.
3. **Vectorize**:
   - Click **"Vectorize The Database"**. This reads your files and creates the
     "brain" for the AI. No need to do it again, unless you want to update the
     database.
4. **Chat**:
   - Ask questions like _"Who has an AWS certificate?"_ and get answers based on
     your files!

## ğŸ§ª Sample Data

Included is a `test_DB` folder with synthetic employee records generated by
**Gemini 2.5 Flash**. You can point the database selector to `test_DB` to test
the app right away!

---

## â“ Troubleshooting

- **`NameError: ChatOllama is not defined`**: Run
  `pip install -r requirements.txt` again to ensure all new dependencies are
  installed.
- **Ollama connection failed**: Ensure the Ollama app is running and you pulled
  `llama3.2`.
- **Missing files**: Check the allowed file types in `ingest.py`.
